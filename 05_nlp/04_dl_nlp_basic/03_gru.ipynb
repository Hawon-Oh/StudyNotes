{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a02dc81",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffc1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_func(train_data, train_label, model):\n",
    "\n",
    "    # 배치 사이즈, 학습/검증셋 크기 설정\n",
    "    BATCH_SIZE = 65\n",
    "    train_size = int(len(train_data) * 0.8)\n",
    "    val_size = len(train_data) - train_size\n",
    "\n",
    "\n",
    "\n",
    "    # 데이터 학습/검증셋 분할\n",
    "    train_dataset, val_dataset = random_split(TensorDataset(train_data, train_label), [train_size, val_size])\n",
    "    # 미니배치로 사용할 수 있도록 DataLoader 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # epoch(학습 횟수), 손실 함수, 최적화 함수 정의\n",
    "    epochs = 100\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "    # 시각화를 위한 손실값/정확도 저장용 배열 생성\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "    # 조기종료 관련 변수 초기화\n",
    "    early_stopping_patience = 7\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # 학습 과정\n",
    "    for epoch in range(epochs):\n",
    "        # 학습 모드\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()               # 가중치 초기화\n",
    "            outputs = model(inputs).squeeze()   # 순전파\n",
    "\n",
    "            loss = criterion(outputs, targets)  # 손실 계산\n",
    "\n",
    "            loss.backward()                     # 역전파\n",
    "            optimizer.step()                    # 가중치 갱신\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred = (outputs > 0.5).float()\n",
    "            correct += (pred == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "            \n",
    "        # 검증 모드\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_outputs = model(val_inputs).squeeze()   # 순전파\n",
    "                loss = criterion(val_outputs, val_targets)  # 손실 계산\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_pred = (val_outputs > 0.5).float()\n",
    "                val_correct += (val_pred == val_targets).sum().item()\n",
    "                val_total += val_targets.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)    \n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # 조기종료 처리\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping!!!\")\n",
    "                break\n",
    "\n",
    "    train_history_df = pd.DataFrame({\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_acc': train_accs,\n",
    "        'val_accs': val_accs\n",
    "    })\n",
    "\n",
    "    train_history_df.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db8b762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 150, 300]), torch.Size([5000, 150, 300]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 300 # 사용할 단어 수\n",
    "SEQ_LEN = 150    # 시퀀스 하나의 최대 길이\n",
    "\n",
    "(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "\n",
    "# Torch Tensor 변환\n",
    "train_input = [torch.tensor(seq, dtype=torch.long) for seq in train_input][:10000] \n",
    "test_input = [torch.tensor(seq, dtype=torch.long) for seq in test_input][:5000] \n",
    "\n",
    "train_target = torch.tensor(train_target, dtype=torch.long)[:10000]\n",
    "test_target = torch.tensor(test_target, dtype=torch.long)[:5000]\n",
    "\n",
    "def pad_sequences(sequences, maxlen, padding_value=0):\n",
    "    padded_sequences = [F.pad(seq[:maxlen], (0, max(0, maxlen-len(seq))), value=padding_value) for seq in sequences]\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "train_seq = pad_sequences(train_input, maxlen=SEQ_LEN)\n",
    "test_seq = pad_sequences(test_input, maxlen=SEQ_LEN)\n",
    "\n",
    "# one-hot encoding\n",
    "train_onehot = F.one_hot(train_seq, num_classes=VOCAB_SIZE).float()\n",
    "test_onehot = F.one_hot(test_seq, num_classes=VOCAB_SIZE).float()\n",
    "\n",
    "# label 데이터 실수 처리\n",
    "train_target = train_target.float()\n",
    "test_target = test_target.float()\n",
    "\n",
    "train_onehot.shape, test_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f3a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, output_dim, \n",
    "                 return_sequences=False, return_state=False):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        self.gru = nn.GRU(input_size=input_dim, hidden_size=hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru()\n",
    "        result_output = self.fc(hidden[-1])\n",
    "\n",
    "        if self.return_sequences and self.return_state:\n",
    "            return output, hidden[-1]\n",
    "        elif self.return_sequences:\n",
    "            return output\n",
    "        elif self.return_state:\n",
    "            return output[:, -1, :], hidden[-1]\n",
    "        \n",
    "        return self.sigmoid(result_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113b3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 8\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "gru_model = GRUModel(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    hidden_units=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd410b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GRU.forward() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgru_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_func\u001b[39m\u001b[34m(train_data, train_label, model)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     42\u001b[39m     optimizer.zero_grad()               \u001b[38;5;66;03m# 가중치 초기화\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m.squeeze()   \u001b[38;5;66;03m# 순전파\u001b[39;00m\n\u001b[32m     45\u001b[39m     loss = criterion(outputs, targets)  \u001b[38;5;66;03m# 손실 계산\u001b[39;00m\n\u001b[32m     47\u001b[39m     loss.backward()                     \u001b[38;5;66;03m# 역전파\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mGRUModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     result_output = \u001b[38;5;28mself\u001b[39m.fc(hidden[-\u001b[32m1\u001b[39m])\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_sequences \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_state:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: GRU.forward() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "train_func(train_onehot, train_target, gru_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe4962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
