{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc134d21",
   "metadata": {},
   "source": [
    "# Padding\n",
    "- 고정된 길이 데이터가 처리에 효율적임. 근데 각 문장 길이가 다름 &rarr; 문장 길이를 동일하게 맞추는 작업 필요\n",
    "\n",
    "**패딩 장점**\n",
    "1. 입력형식 일관화\n",
    "2. 병렬연산 최적화\n",
    "3. 유연한 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d668b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'],\n",
    "                          ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
    "                          ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
    "                          ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "                          ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f4047",
   "metadata": {},
   "source": [
    "### 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823d28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "class TokenizerForPadding:\n",
    "    def __init__(self, num_words=None, oov_token='<OOV>'):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.word_index = {}\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        # 빈도수 세기\n",
    "        for sentence in texts:\n",
    "            self.word_counts.update(word for word in sentence if word)\n",
    "\n",
    "        # 빈도수 기간 vocabulary  생성\n",
    "        vocab = [self.oov_token] + [word for word, _ in self.word_counts.most_common(self.num_words - 2 if self.num_words else None)]\n",
    "\n",
    "        self.word_index = {word: i+1 for i, word in enumerate(vocab)}\n",
    "        self.index_word = {i+1: word for word, i in self.word_index.items()}\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [[self.word_index.get(word, self.word_index[self.oov_token]) for word in sentence] for sentence in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c532a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre', value=0):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(seq) for seq in sequences)\n",
    "\n",
    "        padded_sequences = []\n",
    "\n",
    "        for seq in sequences:\n",
    "            if len(seq) > maxlen:\n",
    "                if truncating == 'pre':\n",
    "                    seq = seq[-maxlen:]\n",
    "                else: # post\n",
    "                    seq = seq[:maxlen]\n",
    "\n",
    "            else:\n",
    "                pad_length = maxlen - len(seq)\n",
    "                if padding == 'pre':\n",
    "                    seq = [value] * pad_length + seq\n",
    "\n",
    "                else:\n",
    "                    seq += [value] * pad_length\n",
    "\n",
    "            padded_sequences.append(seq)    \n",
    "\n",
    "    return torch.tensor(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904e1fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 9, 6],\n",
       " [2, 4, 6],\n",
       " [10, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 7],\n",
       " [2, 5, 7],\n",
       " [2, 5, 3],\n",
       " [8, 8, 4, 3, 11, 2, 12],\n",
       " [2, 13, 4, 14]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerForPadding(num_words=15)\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc40a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  2,  6],\n",
       "        [ 0,  0,  0,  0,  2,  9,  6],\n",
       "        [ 0,  0,  0,  0,  2,  4,  6],\n",
       "        [ 0,  0,  0,  0,  0, 10,  3],\n",
       "        [ 0,  0,  0,  3,  5,  4,  3],\n",
       "        [ 0,  0,  0,  0,  0,  4,  3],\n",
       "        [ 0,  0,  0,  0,  2,  5,  7],\n",
       "        [ 0,  0,  0,  0,  2,  5,  7],\n",
       "        [ 0,  0,  0,  0,  2,  5,  3],\n",
       "        [ 8,  8,  4,  3, 11,  2, 12],\n",
       "        [ 0,  0,  0,  2, 13,  4, 14]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='pre', truncating='pre', maxlen=None)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f805e",
   "metadata": {},
   "source": [
    "### Keras Tokenizer 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ebb68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4100b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0],\n",
       "       [ 1,  8,  5],\n",
       "       [ 1,  3,  5],\n",
       "       [ 9,  2,  0],\n",
       "       [ 2,  4,  3],\n",
       "       [ 3,  2,  0],\n",
       "       [ 1,  4,  6],\n",
       "       [ 1,  4,  6],\n",
       "       [ 1,  4,  2],\n",
       "       [ 7,  7,  3],\n",
       "       [ 1, 12,  3]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=3, truncating='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a950943",
   "metadata": {},
   "source": [
    "##### 어린왕장 패딩\n",
    "1. 텍스트 전처리 (토큰화/불용어/정제/정규화)\n",
    "2. 정수 인코딩 Tokenizer (tensorflow.keras)\n",
    "3. 패딩처리 pad_sequences (tensorflow.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee7a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-Exupéry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
    "\n",
    "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
    "\n",
    "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
    "\n",
    "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
    "\n",
    "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfe115d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prince', 9), ('little', 6), ('pilot', 4), ('rose', 3), ('fox', 3), ('young', 2), ('planet', 2), ('earth', 2), ('story', 2), ('plane', 2), ('meets', 2), ('asteroid', 2), ('lessons', 2), ('love', 2), ('importance', 2), ('written', 1), ('antoine', 1), ('saint-exupéry', 1), ('poetic', 1), ('tale', 1), ('travels', 1), ('home', 1), ('begins', 1), ('stranded', 1), ('sahara', 1), ('desert', 1), ('crashes', 1), ('trying', 1), ('fix', 1), ('mysterious', 1), ('boy', 1), ('comes', 1), ('small', 1), ('called', 1), ('b-612', 1), ('lives', 1), ('alone', 1), ('loves', 1), ('deeply', 1), ('recounts', 1), ('journey', 1), ('describing', 1), ('visits', 1), ('several', 1), ('planets', 1), ('inhabited', 1), ('different', 1), ('character', 1), ('king', 1), ('vain', 1), ('man', 1), ('drunkard', 1), ('businessman', 1), ('geographer', 1), ('encounters', 1), ('learns', 1), ('valuable', 1), ('responsibility', 1), ('nature', 1), ('adult', 1), ('behavior', 1), ('various', 1), ('creatures', 1), ('including', 1), ('teaches', 1), ('relationships', 1), ('taming', 1), ('means', 1), ('building', 1), ('ties', 1), ('others', 1), ('famous', 1), ('line', 1), ('become', 1), ('responsible', 1), ('forever', 1), ('tamed', 1), ('resonates', 1), ('feelings', 1), ('ultimately', 1), ('realizes', 1), ('essence', 1), ('life', 1), ('often', 1), ('invisible', 1), ('seen', 1), ('heart', 1), ('sharing', 1), ('wisdom', 1), ('prepares', 1), ('return', 1), ('beloved', 1), ('concludes', 1), ('reflecting', 1), ('learned', 1), ('enduring', 1), ('impact', 1), ('friendship', 1), ('narrative', 1), ('beautifully', 1), ('simple', 1), ('yet', 1), ('profound', 1), ('exploration', 1), ('loss', 1), ('seeing', 1), ('beyond', 1), ('surface', 1), ('things', 1)]\n",
      "['little prince written antoine saint-exupéry poetic tale young prince travels home planet earth', 'story begins pilot stranded sahara desert plane crashes', 'trying fix plane meets mysterious young boy little prince', 'little prince comes small asteroid called b-612 lives alone rose loves deeply', 'recounts journey pilot describing visits several planets', 'planet inhabited different character king vain man drunkard businessman geographer fox', 'encounters prince learns valuable lessons love responsibility nature adult behavior', 'earth little prince meets various creatures including fox teaches relationships importance taming means building ties others', 'fox famous line become responsible forever tamed resonates prince feelings rose', 'ultimately little prince realizes essence life often invisible seen heart', 'sharing wisdom pilot prepares return asteroid beloved rose', 'story concludes pilot reflecting lessons learned little prince enduring impact friendship', 'narrative beautifully simple yet profound exploration love loss importance seeing beyond surface things']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 문장 토큰\n",
    "sent_tokens = sent_tokenize(raw_text.lower())\n",
    "\n",
    "# 불용어\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "\n",
    "# 사전 (key=단어, value=빈도)\n",
    "vocab = {}\n",
    "\n",
    "# 토큰/정제/정규화\n",
    "preprocessed_sentences = []\n",
    "\n",
    "# 소문자 변환, 토큰화, 불용어, 단어길이 2이하 제거\n",
    "for st in sent_tokens:\n",
    "    temp = ''\n",
    "    for wt in word_tokenize(st):\n",
    "        \n",
    "        if wt not in stopword_set and len(wt) > 2:\n",
    "            if wt in vocab:\n",
    "                vocab[wt] += 1\n",
    "            else:\n",
    "                vocab[wt] = 1\n",
    "\n",
    "            temp += wt + ' '\n",
    "    \n",
    "    preprocessed_sentences.append(temp[:-1])\n",
    "\n",
    "\n",
    "vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print(vocab)\n",
    "print(preprocessed_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'prince', ',', 'written', 'antoine', 'de', 'saint-exupéry', ',', 'poetic', 'tale', 'young', 'prince', 'travels', 'home', 'planet', 'earth', '.']\n",
      "['story', 'begins', 'pilot', 'stranded', 'sahara', 'desert', 'plane', 'crashes', '.']\n",
      "['trying', 'fix', 'plane', ',', 'meets', 'mysterious', 'young', 'boy', ',', 'little', 'prince', '.']\n",
      "['little', 'prince', 'comes', 'small', 'asteroid', 'called', 'b-612', ',', 'lives', 'alone', 'rose', 'loves', 'deeply', '.']\n",
      "['recounts', 'journey', 'pilot', ',', 'describing', 'visits', 'several', 'planets', '.']\n",
      "['planet', 'inhabited', 'different', 'character', ',', 'king', ',', 'vain', 'man', ',', 'drunkard', ',', 'businessman', ',', 'geographer', ',', 'fox', '.']\n",
      "['encounters', ',', 'prince', 'learns', 'valuable', 'lessons', 'love', ',', 'responsibility', ',', 'nature', 'adult', 'behavior', '.']\n",
      "['earth', ',', 'little', 'prince', 'meets', 'various', 'creatures', ',', 'including', 'fox', ',', 'teaches', 'relationships', 'importance', 'taming', ',', 'means', 'building', 'ties', 'others', '.']\n",
      "['fox', \"'s\", 'famous', 'line', ',', '``', 'become', 'responsible', ',', 'forever', ',', 'tamed', ',', \"''\", 'resonates', 'prince', \"'s\", 'feelings', 'rose', '.']\n",
      "['ultimately', ',', 'little', 'prince', 'realizes', 'essence', 'life', 'often', 'invisible', 'seen', 'heart', '.']\n",
      "['sharing', 'wisdom', 'pilot', ',', 'prepares', 'return', 'asteroid', 'beloved', 'rose', '.']\n",
      "['story', 'concludes', 'pilot', 'reflecting', 'lessons', 'learned', 'little', 'prince', 'enduring', 'impact', 'friendship', '.']\n",
      "['narrative', 'beautifully', 'simple', 'yet', 'profound', 'exploration', 'love', ',', 'loss', ',', 'importance', 'seeing', 'beyond', 'surface', 'things', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "# 문장 토큰\n",
    "sent_tokens = sent_tokenize(raw_text.lower())\n",
    "\n",
    "# 불용어\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "\n",
    "# 사전 (key=단어, value=빈도)\n",
    "vocab = {}\n",
    "\n",
    "# 토큰/정제/정규화\n",
    "\n",
    "# 소문자 변환, 토큰화, 불용어, 단어길이 2이하 제거\n",
    "\n",
    "minlen = 3\n",
    "for st in sent_tokens:\n",
    "    word_tokens = [wt for wt in word_tokenize(st) if wt not in punctuation and wt not in stopword_set and len(wt) > minlen]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170b8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   1,  16,  17,  18,  19,  20,  21,   6,   1,  22,  23,   7,\n",
       "          8,   0,   0],\n",
       "       [  9,  24,   3,  25,  26,  27,  10,  28,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [ 29,  30,  10,  11,  31,   6,  32,   2,   1,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  2,   1,  33,  34,  12,  35,  36,  37,  38,  39,   4,  40,  41,\n",
       "          0,   0,   0],\n",
       "       [ 42,  43,   3,  44,  45,  46,  47,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  7,  48,  49,  50,  51,  52,  53,  54,  55,  56,   5,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [ 57,   1,  58,  59,  13,  14,  60,  61,  62,  63,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  8,   2,   1,  11,  64,  65,  66,   5,  67,  68,  15,  69,  70,\n",
       "         71,  72,  73],\n",
       "       [  5,  74,  75,  76,  77,  78,  79,  80,   1,  81,   4,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [ 82,   2,   1,  83,  84,  85,  86,  87,  88,  89,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [ 90,  91,   3,  92,  93,  12,  94,   4,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  9,  95,   3,  96,  13,  97,   2,   1,  98,  99, 100,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [101, 102, 103, 104, 105, 106,  14, 107,  15, 108, 109, 110, 111,\n",
       "          0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# num_words: 단어사건에 사용할 개수\n",
    "# oov_token: OOV토큰 명으로 사용할 이름 지정\n",
    "tokenizer = Tokenizer(num_words=15, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, padding='post', truncating='post')\n",
    "padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
