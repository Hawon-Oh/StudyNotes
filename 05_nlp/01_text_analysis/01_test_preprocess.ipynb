{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae1edb1",
   "metadata": {},
   "source": [
    "# 자연어 처리 NLP | 텍스트 분석 Text Analysis\n",
    "- 자연어 처리: 언어를 이해하고 처리 (번역, 음성인식, 언어생성 등)\n",
    "- 텍스트 분석: 텍스트에서 정보 추출 및 분석 (텍스트 통계분석, 주제 분류, 텍스트 군집, 유사도 분석 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b832aa",
   "metadata": {},
   "source": [
    "### 파이썬 텍스트분석 패키지 \n",
    "\n",
    "| **로고 이미지**                                                                                                 | **패키지**   | **설명**                                            | **주요 특징 및 기능**                                                   | **API 문서 URL**                                  |\n",
    "|------------------------------------------------------------------------------------------------------------|--------------|---------------------------------------------------|-----------------------------------------------------------------------|-------------------------------------------------|\n",
    "| ![nltk](https://d.pr/i/9xVzCK+)                   | **nltk**     | 가장 오래된 NLP 라이브러리 중 하나로, 다양한 자연어 처리 도구와 코퍼스 제공 | 토큰화, 품사 태깅, 어간 추출, 불용어 제거, 문법 구조 분석, 감정 분석 등에 유용 | [NLTK API Docs](https://www.nltk.org/api/nltk.html) |\n",
    "| ![gensim](https://radimrehurek.com/gensim/_static/images/gensim.png)                                       | **gensim**   | 주로 텍스트의 토픽 모델링과 문서 유사도 분석을 위한 라이브러리            | Word2Vec, FastText, LDA, 유사도 측정, 대용량 텍스트 처리에 최적화    | [Gensim API Docs](https://radimrehurek.com/gensim/) |\n",
    "| ![spacy](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/320px-SpaCy_logo.svg.png) | **spacy**    | 빠르고 효율적인 NLP 처리를 위해 개발된 라이브러리로, 산업용 프로젝트에 적합     | 빠른 토큰화, 품사 태깅, NER, 구문 분석, 벡터 표현 제공              | [SpaCy API Docs](https://spacy.io/api)             |\n",
    "| ![TextBlob](https://textblob.readthedocs.io/en/dev/_static/textblob-logo.png)                              | **TextBlob** | 간단한 NLP 작업을 위한 라이브러리로, 감정 분석과 텍스트 정제 등 지원  | 문법 교정, 감정 분석, 텍스트 번역 등과 같은 간단한 작업에 적합      | [TextBlob API Docs](https://textblob.readthedocs.io/en/dev/) |\n",
    "| ![KoNLPy](https://konlpy.org/en/latest/_static/konlpy.png)                                                 | **KoNLPy**   | 한국어 자연어 처리를 위한 라이브러리로, 여러 형태소 분석기를 제공          | Kkma, Hannanum, Komoran, Twitter, Mecab 형태소 분석기 지원            | [KoNLPy API Docs](https://konlpy.org/en/latest/)  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b520da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Retrieving notices: done\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Playdata\\anaconda3\\envs\\ml_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    click-8.2.1                |  py312haa95532_0         329 KB\n",
      "    colorama-0.4.6             |  py312haa95532_0          53 KB\n",
      "    joblib-1.5.2               |  py312haa95532_0         517 KB\n",
      "    nltk-3.9.1                 |  py312haa95532_0         2.7 MB\n",
      "    openssl-3.0.18             |       h543e019_0         6.8 MB\n",
      "    regex-2025.9.1             |  py312h02ab6af_0         364 KB\n",
      "    tqdm-4.67.1                |  py312hfc267ef_0         187 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        11.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  click              pkgs/main/win-64::click-8.2.1-py312haa95532_0 \n",
      "  colorama           pkgs/main/win-64::colorama-0.4.6-py312haa95532_0 \n",
      "  joblib             pkgs/main/win-64::joblib-1.5.2-py312haa95532_0 \n",
      "  nltk               pkgs/main/win-64::nltk-3.9.1-py312haa95532_0 \n",
      "  regex              pkgs/main/win-64::regex-2025.9.1-py312h02ab6af_0 \n",
      "  tqdm               pkgs/main/win-64::tqdm-4.67.1-py312hfc267ef_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2025.7.15-haa95532_0 --> 2025.9.9-haa95532_0 \n",
      "  openssl                                 3.0.17-h35632f6_0 --> 3.0.18-h543e019_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "openssl-3.0.18       | 6.8 MB    |            |   0% \n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    |            |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    |            |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    |            |   0% \n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    | 4          |   4% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    | 6          |   6% \u001b[A\u001b[A\n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    | 1          |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    | 4          |   5% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    | ##1        |  21% \n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    | ######3    |  63% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    | 8          |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     | ###        |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    | ####7      |  47% \n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    | ########## | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "regex-2025.9.1       | 364 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    | ########## | 100% \n",
      "openssl-3.0.18       | 6.8 MB    | ########## | 100% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "colorama-0.4.6       | 53 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "click-8.2.1          | 329 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tqdm-4.67.1          | 187 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "joblib-1.5.2         | 517 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "openssl-3.0.18       | 6.8 MB    | ########## | 100% \n",
      "\n",
      "nltk-3.9.1           | 2.7 MB    | ########## | 100% \u001b[A\n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.5.1\n",
      "    latest version: 25.9.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install nltk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5462985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15067bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')      # 토큰화에 필요한 데아터\n",
    "nltk.download('punkt_tab')  # 구두점, 점 규칙\n",
    "nltk.download('stopwords')  # 불용어 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab23dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waes', '32ert', '3re', '3']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = 'waes 32ert 3re 3'\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74ac7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정분석\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cdd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.973, 'neu': 0.027, 'pos': 0.0, 'compound': -0.9996}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "analyzer.polarity_scores('I don\\'t  HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATE hate trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95b5e813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i love this', {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}),\n",
       " ('monkey', {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}),\n",
       " ('kimbab lover hate tuna kimbab',\n",
       "  {'neg': 0.352, 'neu': 0.286, 'pos': 0.362, 'compound': 0.0258})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['i love this', 'monkey', 'kimbab lover hate tuna kimbab']\n",
    "\n",
    "[(text, analyzer.polarity_scores(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10230063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATEHATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATEHATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATEHATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATEHATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'HATE',\n",
       " 'hate',\n",
       " 'trump']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화 결과가 감정분석에 미치는 영향\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "corpus = 'I don\\'t  HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATE hate trump'\n",
    "analyzer\n",
    "\n",
    "# WordPunctToKenizer: 특수문자도 토큰처리\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "tokens1 = word_punct_tokenizer.tokenize(corpus)\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23ba88c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATEHATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATEHATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATEHATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATEHATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'HATE', 'hate', 'trump']\n"
     ]
    }
   ],
   "source": [
    "token2 = word_tokenize(corpus)\n",
    "print(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5004e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don ' t HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATE hate trump\n",
      "{'neg': 0.968, 'neu': 0.032, 'pos': 0.0, 'compound': -0.9996}\n"
     ]
    }
   ],
   "source": [
    "text1 = ' '.join(tokens1)\n",
    "print(text1)\n",
    "print(analyzer.polarity_scores(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcf14e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do n't HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATEHATE HATE HATE HATE HATE HATE HATE HATE hate trump\n",
      "{'neg': 0.024, 'neu': 0.046, 'pos': 0.93, 'compound': 0.9992}\n"
     ]
    }
   ],
   "source": [
    "text2 = ' '.join(token2)\n",
    "print(text2)\n",
    "print(analyzer.polarity_scores(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c978688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Matrix is everywhere its all around us, here even in this room.',\n",
       " 'You can see it out your window or on your television.',\n",
       " 'You feel it when you go to work, or go to church or pay your taxes.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''The Matrix is everywhere its all around us, here even in this room.\n",
    "You can see it out your window or on your television.\n",
    "You feel it when you go to work, or go to church or pay your taxes.'''\n",
    "\n",
    "sent_tokenize(text) # 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ff24769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장별 단어 토큰화\n",
    "def sentences_tokenize(sentences):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [word_tokenize(s) for s in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33c3883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dawesrdfd', 'fgdhdfd'), ('fgdhdfd', 'fgdgff')]\n",
      "[('dawesrdfd', 'fgdhdfd', 'fgdgff')]\n"
     ]
    }
   ],
   "source": [
    "# n-gram\n",
    "from nltk import ngrams\n",
    "\n",
    "text = 'dawesrdfd fgdhdfd fgdgff'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "bigram = ngrams(tokens, 2)\n",
    "print([token for token in bigram])\n",
    "\n",
    "trigram = ngrams(tokens, 3)\n",
    "print([token for token in trigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b382e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# stopwords.fileids() # available langs\n",
    "print(stopwords.words('English')[:10]) # 불용어 목록\n",
    "print(len(stopwords.words('English')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9febe8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick brown fox jumps lazy dog\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is reshaping the future of technology.\",\n",
    "    \"Data without context is meaningless.\",\n",
    "    \"Go is designed for simplicity, concurrency, and performance.\",\n",
    "    \"Python remains the dominant language for machine learning.\",\n",
    "    \"Distributed systems require careful management of state and latency.\",\n",
    "    \"Sustainable agriculture can benefit from predictive AI models.\",\n",
    "    \"The server responded with a 500 internal error.\",\n",
    "    \"Version control allows developers to collaborate safely.\",\n",
    "    \"Training deep networks demands both data and computational power.\",\n",
    "    \"Quantum computing may redefine the boundaries of encryption.\",\n",
    "    \"Ethical AI must prioritize transparency and accountability.\",\n",
    "    \"A scalable backend should handle thousands of concurrent users.\",\n",
    "    \"The model’s performance improved after fine-tuning the parameters.\",\n",
    "    \"React components make it easier to manage UI state.\"\n",
    "]\n",
    "\n",
    "text = corpus[0]\n",
    "\n",
    "stopwords_list = set(stopwords.words('English'))\n",
    "\n",
    "tokens = []\n",
    "\n",
    "\n",
    "\n",
    "# 소문자 변환\n",
    "text = text.lower()\n",
    "\n",
    "# 토큰화\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "new_token = \"\"\n",
    "# 불용어 처리\n",
    "for token in tokens:\n",
    "    if token not in stopwords_list:\n",
    "        if token.isalpha() or token.isdigit():\n",
    "            new_token += token + \" \"\n",
    "\n",
    "new_token = new_token[:-1]\n",
    "print(new_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2d38a",
   "metadata": {},
   "source": [
    "### 특성 벡터화 Feature Vectorization\n",
    "1. BOW(Bag of Words): 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델이다.\n",
    "    \n",
    "   <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*S8uW62e3AbYw3gnylm4eDg.png\" width=\"500px\">\n",
    "\n",
    "2. Word Embedding: 단어를 밀집 벡터(dense vector)로 표현하는 방법으로, 단어의 의미와 관계를 보존하며 벡터로 표현한다.\n",
    "    \n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*jpnKO5X0Ii8PVdQYFO2z1Q.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "| **구분**             | **Bag-of-Words (BOW)**                             | **Word Embedding**                             |\n",
    "|----------------------|--------------------------------------------------|------------------------------------------------|\n",
    "| **개념**             | 문서를 단어의 출현 빈도로 표현                   | 단어를 실수 벡터로 표현                       |\n",
    "| **특징**             | - 단어의 순서와 의미를 고려하지 않음             | - 단어 간 의미적 유사성을 반영                |\n",
    "|                      | - 고차원, 희소 벡터 생성                         | - 밀집된 저차원 벡터 생성                     |\n",
    "| **대표 방법**        | Count Vector, TF-IDF                             | Word2Vec, GloVe, FastText                     |\n",
    "| **장점**             | - 구현이 간단하고 이해하기 쉬움                  | - 문맥 정보 반영 가능                         |\n",
    "|                      | - 단순 텍스트 데이터 분석에 유용                 | - 유사한 단어를 벡터 공간 상에서 가깝게 위치 |\n",
    "| **단점**             | - 의미적 관계와 단어의 순서 정보 없음            | - 많은 데이터와 학습 시간 필요                |\n",
    "|                      | - 고차원 희소 벡터 문제                          | - 구현이 상대적으로 복잡                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416b2aa",
   "metadata": {},
   "source": [
    "- BOW > CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "767167af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "text2 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe \\\n",
    "You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "\n",
    "texts = [text1, text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e33f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 1, 2, 1,\n",
       "        1, 1, 3, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 3, 3],\n",
       "       [0, 4, 0, 1, 2, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 2, 0, 0, 4, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1, 0, 7, 1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(texts) # 고유한 단어(토큰)을 추출하여 단어사전 생성\n",
    "text_vecs = count_vectorizer.transform(texts)\n",
    "\n",
    "print(type(text_vecs), type(text_vecs.toarray()))\n",
    "text_vecs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "078b2981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'and' 'around' 'bed' 'believe' 'blue' 'can' 'church' 'deep' 'ends'\n",
      " 'even' 'everywhere' 'feel' 'go' 'goes' 'here' 'hole' 'how' 'in' 'is' 'it'\n",
      " 'its' 'matrix' 'on' 'or' 'out' 'pay' 'pill' 'rabbit' 'red' 'room' 'see'\n",
      " 'show' 'stay' 'story' 'take' 'taxes' 'television' 'the' 'this' 'to' 'us'\n",
      " 'wake' 'want' 'whatever' 'when' 'window' 'wonderland' 'work' 'you' 'your']\n",
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names_out())\n",
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f182a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your',\n",
       " 'you',\n",
       " 'work',\n",
       " 'wonderland',\n",
       " 'window',\n",
       " 'when',\n",
       " 'whatever',\n",
       " 'want',\n",
       " 'wake',\n",
       " 'us',\n",
       " 'to',\n",
       " 'this',\n",
       " 'the',\n",
       " 'television',\n",
       " 'taxes',\n",
       " 'take',\n",
       " 'story',\n",
       " 'stay',\n",
       " 'show',\n",
       " 'see',\n",
       " 'room',\n",
       " 'red',\n",
       " 'rabbit',\n",
       " 'pill',\n",
       " 'pay',\n",
       " 'out',\n",
       " 'or',\n",
       " 'on',\n",
       " 'matrix',\n",
       " 'its',\n",
       " 'it',\n",
       " 'is',\n",
       " 'in',\n",
       " 'how',\n",
       " 'hole',\n",
       " 'here',\n",
       " 'goes',\n",
       " 'go',\n",
       " 'feel',\n",
       " 'everywhere',\n",
       " 'even',\n",
       " 'ends',\n",
       " 'deep',\n",
       " 'church',\n",
       " 'can',\n",
       " 'blue',\n",
       " 'believe',\n",
       " 'bed',\n",
       " 'around',\n",
       " 'and',\n",
       " 'all']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(count_vectorizer.vocabulary_, key=lambda x: x, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e477a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>around</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bed</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  idx\n",
       "0      all    0\n",
       "1      and    1\n",
       "2   around    2\n",
       "3      bed    3\n",
       "4  believe    4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "vocab_df = pd.DataFrame(vocab, columns=['word', 'idx'])\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f6a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>around</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blue</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>can</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>church</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deep</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ends</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>even</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>everywhere</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>feel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>go</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>goes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hole</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>is</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>its</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>matrix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>on</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>or</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>out</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pill</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>room</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>see</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>show</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>stay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>story</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>take</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>taxes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>television</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>this</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>to</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>whatever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>when</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>window</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>wonderland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>you</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>your</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0          all      1\n",
       "1          and      4\n",
       "2       around      1\n",
       "3          bed      1\n",
       "4      believe      2\n",
       "5         blue      1\n",
       "6          can      1\n",
       "7       church      1\n",
       "8         deep      1\n",
       "9         ends      1\n",
       "10        even      1\n",
       "11  everywhere      1\n",
       "12        feel      1\n",
       "13          go      2\n",
       "14        goes      1\n",
       "15        here      1\n",
       "16        hole      1\n",
       "17         how      1\n",
       "18          in      3\n",
       "19          is      1\n",
       "20          it      2\n",
       "21         its      1\n",
       "22      matrix      1\n",
       "23          on      1\n",
       "24          or      3\n",
       "25         out      1\n",
       "26         pay      1\n",
       "27        pill      2\n",
       "28      rabbit      1\n",
       "29         red      1\n",
       "30        room      1\n",
       "31         see      1\n",
       "32        show      1\n",
       "33        stay      1\n",
       "34       story      1\n",
       "35        take      2\n",
       "36       taxes      1\n",
       "37  television      1\n",
       "38         the      5\n",
       "39        this      1\n",
       "40          to      3\n",
       "41          us      1\n",
       "42        wake      1\n",
       "43        want      1\n",
       "44    whatever      1\n",
       "45        when      1\n",
       "46      window      1\n",
       "47  wonderland      1\n",
       "48        work      1\n",
       "49         you     10\n",
       "50        your      4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어별 등장횟수\n",
    "word_counts = text_vecs.toarray().sum(axis=0)\n",
    "\n",
    "vocab_df['count'] = vocab_df['idx'].apply(lambda i: word_counts[i])\n",
    "vocab_df = vocab_df.drop(columns=['idx'])\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2521080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 24)\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer(stop_words='언어'): 불용어 제거\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "texts_vecs = count_vectorizer.fit_transform(texts)\n",
    "print(texts_vecs.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5c14e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>church</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  idx\n",
       "0      bed    0\n",
       "1  believe    1\n",
       "2     blue    2\n",
       "3   church    3\n",
       "4     deep    4"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "vocab_df = pd.DataFrame(vocab, columns=['word', 'idx'])\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f61cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bed', 'bed believe', 'believe', 'believe red', 'believe want',\n",
       "       'blue', 'blue pill', 'church', 'church pay', 'deep', 'deep rabbit',\n",
       "       'ends', 'ends wake', 'feel', 'feel work', 'goes', 'hole',\n",
       "       'hole goes', 'matrix', 'matrix room', 'pay', 'pay taxes', 'pill',\n",
       "       'pill stay', 'pill story', 'rabbit', 'rabbit hole', 'red',\n",
       "       'red pill', 'room'], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer(stop_words='언어'): 불용어 제거\n",
    "count_vectorizer = CountVectorizer(stop_words='english',\n",
    "                                   ngram_range=(1, 2),      # 범위 1 ~ 2\n",
    "                                   max_features=30          # 빈도수가 높은 n개만 사용\n",
    "                                   )\n",
    "\n",
    "\n",
    "texts_vecs = count_vectorizer.fit_transform(texts)\n",
    "print(texts_vecs.toarray().shape)\n",
    "\n",
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67888bf",
   "metadata": {},
   "source": [
    "- BOW > TfidfVectorizer\n",
    "\n",
    "문장 안에선 많아야 중요하고 전체 문장에선 적어야 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bff92",
   "metadata": {},
   "source": [
    "- TF-IDF == Term Frequency-Inverse Document Frequency\n",
    "\n",
    "**용어** \n",
    "- $tf(t, d)$: 특정 단어 $t$가 문서 $d$에서 등장한 횟수 (Term Frequency)\n",
    "- $df(t)$: 특정 단어 $t$가 등장한 문서의 수 (Document Frequency)\n",
    "- $N$: 전체 문서의 수\n",
    "\n",
    "**TF (Term Frequency)**\n",
    "- 단어 $t$의 문서 $d$에서의 빈도를 계산하는데, 가장 일반적인 방법은 해당 단어의 단순 빈도로 정의한다.\n",
    "\n",
    "$\n",
    "tf(t, d) = \\frac{\\text{단어 } t \\text{의 문서 } d \\text{ 내 등장 횟수}}{\\text{문서 } d \\text{의 전체 단어 수}}\n",
    "$\n",
    "\n",
    "**IDF (Inverse Document Frequency)**\n",
    "- 단어가 전체 문서에서 얼마나 중요한지를 계산한다.\n",
    "- 특정 단어가 많은 문서에서 등장하면, 이 단어는 중요도가 낮아진다. 이를 반영하기 위해 아래와 같은 식을 사용한다.\n",
    "\n",
    "$\n",
    "idf(t) = \\log\\left(\\frac{N}{1 + df(t)}\\right)\n",
    "$\n",
    "\n",
    "- 여기서 $1$을 더하는 이유는, 특정 단어가 모든 문서에 등장하지 않을 경우 $df(t) = 0$이 되어, 분모가 $0$이 되는 것을 방지하기 위함이다.\n",
    "  - 예를 들어, $\\log(5/(1+1))$과 $\\log(5/(1+2))$를 계산하면, 각각 $0.3979$와 $0.2218$이 된다.\n",
    "\n",
    "**TF-IDF 계산**\n",
    "- 위의 TF와 IDF를 결합하여 TF-IDF 가중치를 계산한다.\n",
    "\n",
    "$\n",
    "\\text{tf-idf}(t, d) = tf(t, d) \\times idf(t)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060ae0f",
   "metadata": {},
   "source": [
    "**TfidfVectorizer의 주요 파라미터**\n",
    "<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
    "  <tr>\n",
    "    <th>Parameter</th>\n",
    "    <th>Description</th>\n",
    "    <th>Default Value</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>max_df</b></td>\n",
    "    <td>문서의 비율 값으로서, 해당 비율 이상 나타나는 단어를 무시한다. <br> 예를 들어, max_df=0.8이면, 80% 이상의 문서에서 나타나는 단어는 제외된다.</td>\n",
    "    <td>1.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>min_df</b></td>\n",
    "    <td>문서의 비율 값 또는 정수로, 해당 비율 이하 나타나는 단어를 무시한다. <br> 예를 들어, min_df=2이면, 두 개 이하의 문서에서만 나타나는 단어는 제외된다.</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>ngram_range</b></td>\n",
    "    <td>(min_n, max_n) 형식으로, 사용할 n-gram의 범위를 정의한다. <br> 예를 들어, (1, 2)로 설정하면 unigram과 bigram을 고려한다.</td>\n",
    "    <td>(1, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>stop_words</td>\n",
    "    <td>불용어를 지정할 수 있다. \"english\"로 설정하면 영어 불용어를 사용한다.</td>\n",
    "    <td>None</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>max_features</td>\n",
    "    <td>벡터화할 때 고려할 최대 단어 수를 설정한다. 빈도순으로 상위 단어들이 선택된다.</td>\n",
    "    <td>None</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>use_idf</td>\n",
    "    <td>IDF(역문서 빈도)를 사용할지 여부를 지정한다. False로 설정하면 단순히 TF 값만 사용한다.</td>\n",
    "    <td>True</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>smooth_idf</td>\n",
    "    <td>IDF 계산 시, 0으로 나누는 것을 피하기 위해 추가적인 smoothing을 수행한다.</td>\n",
    "    <td>True</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sublinear_tf</td>\n",
    "    <td>TF 값에 대해 sublinear scaling (1 + log(tf))를 적용할지 지정한다.</td>\n",
    "    <td>False</td>\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c622728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.33333333 0.         0.\n",
      "  0.33333333 0.         0.         0.33333333 0.33333333 0.\n",
      "  0.         0.         0.33333333 0.         0.         0.33333333\n",
      "  0.33333333 0.         0.         0.33333333 0.         0.33333333]\n",
      " [0.21821789 0.43643578 0.21821789 0.         0.21821789 0.21821789\n",
      "  0.         0.21821789 0.21821789 0.         0.         0.43643578\n",
      "  0.21821789 0.21821789 0.         0.21821789 0.21821789 0.\n",
      "  0.         0.21821789 0.21821789 0.         0.21821789 0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bed', 'believe', 'blue', 'church', 'deep', 'ends', 'feel', 'goes',\n",
       "       'hole', 'matrix', 'pay', 'pill', 'rabbit', 'red', 'room', 'stay',\n",
       "       'story', 'taxes', 'television', 'wake', 'want', 'window',\n",
       "       'wonderland', 'work'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "texts_vecs = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "print(texts_vecs.toarray())\n",
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
