{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b9cb90",
   "metadata": {},
   "source": [
    "### 미분\n",
    "- 미분의 기본은 함수의 변화율을 구하는거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb42a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**오차역전파의 단계**\n",
    "1. **순전파(Forward Propagation)**: 입력 데이터가 네트워크를 통과하며 예측값을 생성한다.\n",
    "2. **오차 계산(Error Calculation)**: 예측값과 실제 목표값 사이의 오차를 계산한다. 대표적인 오차 함수는 평균 제곱 오차(MSE)이다.\n",
    "3. **오차 역전파(Backpropagation)**: 오차를 네트워크의 각 가중치로 전파하여 가중치를 조정한다.\n",
    "4. **가중치 갱신(Update Weights)**: 경사하강법을 통해 오차가 감소하도록 각 가중치를 갱신한다.\n",
    "\n",
    "\n",
    "**간단히 단일 계층(입력 $ x $ -> 은닉층 $ h $ -> 출력 $ y $) 신경망에서 오차역전파 설명**\n",
    "\n",
    "![](https://d.pr/i/juFjHc+)\n",
    "\n",
    "1. **순전파 단계**\n",
    "\n",
    "    - 가중치 $ w_1 $와 $ w_2 $가 각각 입력과 은닉층, 은닉층과 출력층 사이에 존재한다고 하자.\n",
    "    - 입력 $ x $가 은닉층 $ h $에 도달하면서 가중치 $ w_1 $을 곱한 뒤 활성화 함수를 적용:\n",
    "      $\n",
    "      h = f(x \\cdot w_1)\n",
    "      $\n",
    "    - 은닉층 출력 $ h $가 출력층 $ y $로 전달되며 가중치 $ w_2 $를 곱한 뒤 활성화 함수를 적용하여 최종 출력:\n",
    "      $\n",
    "      y = f(h \\cdot w_2)\n",
    "      $\n",
    "\n",
    "2. **오차 계산**\n",
    "\n",
    "    - 예측된 출력 $ y $와 목표 값 $ t $ 사이의 오차 $ E $를 계산한다. 여기서 평균 제곱 오차(MSE)를 사용하면:\n",
    "      $\n",
    "      E = \\frac{1}{2}(t - y)^2\n",
    "      $\n",
    "\n",
    "3. **오차의 기울기 계산**\n",
    "\n",
    "    - 오차 $ E $를 최소화하기 위해 각 가중치 $ w_1 $과 $ w_2 $에 대한 편미분을 구해야 한다.\n",
    "    - 출력층의 오차 기울기:\n",
    "      $\n",
    "      \\frac{\\partial E}{\\partial w_2} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_2}\n",
    "      $\n",
    "    - 은닉층의 오차 기울기:\n",
    "      $\n",
    "      \\frac{\\partial E}{\\partial w_1} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h} \\cdot \\frac{\\partial h}{\\partial w_1}\n",
    "      $\n",
    "\n",
    "4. **가중치 갱신**\n",
    "\n",
    "    - 각 가중치는 학습률 $ \\eta $와 오차 기울기를 사용해 갱신한다:\n",
    "      $\n",
    "      w_2 = w_2 - \\eta \\cdot \\frac{\\partial E}{\\partial w_2}\n",
    "      $\n",
    "      $\n",
    "      w_1 = w_1 - \\eta \\cdot \\frac{\\partial E}{\\partial w_1}\n",
    "      $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002a3ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 손실함수/활성화함수의 도함수\n",
    "\n",
    "\n",
    "### 손실 함수 (Loss Functions)의 도함수\n",
    "\n",
    "| 손실 함수 | 공식 | 도함수 | 비고 |\n",
    "|-----------|------|--------|------|\n",
    "| **MSELoss** (Mean Squared Error) | $L = \\frac{1}{n} \\sum (y - \\hat{y})^2$ | $\\frac{dL}{d\\hat{y}} = \\frac{2}{n} (\\hat{y} - y)$ | 예측값과 정답의 차이를 제곱해 평균, **이상치에 민감함** |\n",
    "| **L1Loss** (Mean Absolute Error) | $L = \\frac{1}{n} \\sum \\|y - \\hat{y}\\|$ | $\\frac{dL}{d\\hat{y}} = \\frac{1}{n} \\cdot \\text{sign}(\\hat{y} - y)$ | 절댓값 오차 평균, **이상치에 덜 민감**하지만 미분 불연속 |\n",
    "| **HuberLoss** | $L = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } \\|y - \\hat{y}\\| \\leq \\delta \\\\ \\delta \\cdot (\\|y - \\hat{y}\\| - \\frac{1}{2} \\delta) & \\text{otherwise} \\end{cases}$ | $\\frac{dL}{d\\hat{y}} = \\begin{cases} \\hat{y} - y & \\text{if } \\|\\hat{y} - y\\| \\leq \\delta \\\\ \\delta \\cdot \\text{sign}(\\hat{y} - y) & \\text{otherwise} \\end{cases}$ | MSE와 MAE의 장점 결합, **이상치에 덜 민감하면서 부드러운 미분** |\n",
    "| **BCELoss** (Binary Cross Entropy) | $L = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right)$ | $\\frac{dL}{d\\hat{y}} = -\\left( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right)$ | **이진 분류**에서 사용, 출력에 **sigmoid**를 적용해야 함 |\n",
    "| **BCEWithLogitsLoss** | $L = \\max(z, 0) - z \\cdot y + \\log(1 + e^{-\\|z\\|})$ | $\\frac{dL}{dz} = \\sigma(z) - y$ | sigmoid + BCELoss 결합 형태, **수치적으로 안정적** |\n",
    "| **CrossEntropyLoss** | $L = - \\log \\left( \\frac{e^{z_y}}{\\sum_j e^{z_j}} \\right )$ | $\\frac{dL}{dz_i} = \\text{softmax}(z_i) - y_i$ | 다중 클래스 분류용, 내부에서 softmax 포함, **출력에 softmax 불필요** |\n",
    "\n",
    "\n",
    "\n",
    "### 활성화 함수 (Activation Functions)의 도함수\n",
    "\n",
    "\n",
    "| 함수 이름 | 공식 | 도함수 | 비고 |\n",
    "|-----------|------|--------|------|\n",
    "| **Sigmoid** | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ | $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$ | 이진 분류 확률 출력에 주로 사용 |\n",
    "| **Softmax** | $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | $\\frac{\\partial s_i}{\\partial x_j} = s_i (\\delta_{ij} - s_j)$ | 다중 클래스 확률 출력, 출력 간 상호작용 있음 |\n",
    "| **ReLU** | $ReLU(x) = \\max(0, x)$ | $ReLU'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$ | 계산 간단하고 많이 사용됨 |\n",
    "| **LeakyReLU** | $LeakyReLU(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{otherwise} \\end{cases}$ | $LeakyReLU'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha & \\text{otherwise} \\end{cases}$ | 음수 영역도 일부 통과시켜 죽은 ReLU 문제 완화 |\n",
    "| **Tanh** | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $\\tanh'(x) = 1 - \\tanh^2(x)$ | 출력이 -1~1 범위로 중심 정규화 효과 |\n",
    "| **Softmax + CrossEntropy** | $L = -\\sum_i y_i \\log(s_i)$ | $\\frac{\\partial L}{\\partial x_i} = s_i - y_i$ | softmax와 cross-entropy를 결합한 경우, 도함수가 매우 간단해짐 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcf57d",
   "metadata": {},
   "source": [
    "### 연쇄 법칙\n",
    "- 기본 수식의 역전파 & 연쇄법칙 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86acd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward(x):\n",
    "    y = x ** 2\n",
    "    return y\n",
    "\n",
    "def backward(x):\n",
    "    dy_dx = 2 * x\n",
    "    return dy_dx\n",
    "\n",
    "x = 3.0\n",
    "print(forward(x))\n",
    "print(backward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ba5e5",
   "metadata": {},
   "source": [
    "- 다층 신경망에서 연쇄법칙 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb75d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# x -> y -> z\n",
    "def forward(x):\n",
    "    y = x ** 2\n",
    "    z = 2 * y\n",
    "    return z\n",
    "\n",
    "def backward(x):\n",
    "    dy_dx = 2 * x\n",
    "    dz_dy = 2\n",
    "    dz_dx =  dy_dx * dz_dy\n",
    "    return dz_dx\n",
    "\n",
    "x = 3.0\n",
    "print(forward(x))\n",
    "print(backward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6eee1",
   "metadata": {},
   "source": [
    "### 신경망에서의 활용\n",
    "- 단순 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beeab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_d(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "X = np.array([0.5, 0.8])\n",
    "y = np.array([1])\n",
    "\n",
    "W = np.array([0.2, 0.4])\n",
    "\n",
    "# 순전파\n",
    "z = np.dot(X, W)\n",
    "r = sigmoid(z)\n",
    "\n",
    "# 오차 계산\n",
    "loss = 0.5 * ((y - r) ** 2)\n",
    "\n",
    "# 역전파 (기울기 계산)\n",
    "delta = (r - y) * sigmoid_d(z)\n",
    "grad_w = delta * x\n",
    "\n",
    "\n",
    "# 가중치 갱신\n",
    "W -= 0.1 * grad_w # 0.1 == learning_rate\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4676e72",
   "metadata": {},
   "source": [
    "- 은닉층 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfefdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_d(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "X = np.array([0.5, 0.8]) # (2,)\n",
    "y = np.array([1])        # (1,)\n",
    "\n",
    "W1 = np.array([[0.2, 0.4], [0.1, 0.3]]) # (2, 2)\n",
    "b1 = np.array([0.1, 0.2])               # (2,)\n",
    "W2 = np.array([[0.5], [0.6]])           # (2, 1)\n",
    "b2 = np.array([0.3]) # (1,)             # (1,)\n",
    "\n",
    "# 순전파\n",
    "z1 = np.dot(X, W1) + b1\n",
    "r1 = relu(z1)\n",
    "\n",
    "z2 = np.dot(r1, W2) + b2\n",
    "r2 = relu(z2) \n",
    "\n",
    "# 손실 계산 -> 역전파 (기울기 계산)\n",
    "delta2 = (r2 - y) * relu_d(z2)\n",
    "grad_W2 = np.outer(r1, delta2)\n",
    "delta1 = np.dot(W2, delta2) * relu_d(z1)\n",
    "grad_W1 = grad_W1 = np.outer(X, delta1)\n",
    "\n",
    "# 가중치 갱신\n",
    "learning_rate = 0.01\n",
    "W2 -= learning_rate * grad_W2\n",
    "W1 -= learning_rate * grad_W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a595b423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  5]\n",
      " [ 8 10]\n",
      " [12 15]]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([1, 2, 3])\n",
    "arr2 = np.array([4, 5])\n",
    "\n",
    "print(np.outer(arr1, arr2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4edab",
   "metadata": {},
   "source": [
    "### 수치미분과 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c6fa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.000000000039306\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "def num_d_gradient(f, x):\n",
    "    h = 1e-5\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def backward_gradient(x):\n",
    "    return 2 * x\n",
    "\n",
    "print(num_d_gradient(f, 3.0))\n",
    "print(backward_gradient(3.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ceb2e2",
   "metadata": {},
   "source": [
    "##### 숫자 맞추기 AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8685c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 | 예측값 41.99884279997745 | 손실 8.266122791221897e-07\n",
      "epoch 200 | 예측값 41.99999996926315 | 손실 5.831813228917597e-16\n",
      "epoch 300 | 예측값 41.99999999999918 | 손실 4.1359030627651384e-25\n",
      "epoch 400 | 예측값 41.99999999999997 | 손실 4.0389678347315804e-28\n",
      "epoch 500 | 예측값 41.99999999999997 | 손실 4.0389678347315804e-28\n",
      "최종 예측값 41.99999999999997\n"
     ]
    }
   ],
   "source": [
    "target_number = 42\n",
    "guess = np.random.randn()\n",
    "learning_rate = 0.1\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 오차계산\n",
    "    loss = (guess - target_number) ** 2 / 2\n",
    "    \n",
    "    # 역전파 (기울기 계산)\n",
    "    grad = (guess - target_number)\n",
    "\n",
    "    # 업데이트 (guess 업데이트)\n",
    "    guess -= learning_rate * grad\n",
    "\n",
    "    # epoch 100마다 예측값과 손실 출력\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'epoch {epoch + 1} | 예측값 {guess} | 손실 {loss}')\n",
    "\n",
    "# 최종 예측값 guess 출력\n",
    "print(f'최종 예측값 {guess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd1ab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3159468043686356\n"
     ]
    }
   ],
   "source": [
    "print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f77622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
