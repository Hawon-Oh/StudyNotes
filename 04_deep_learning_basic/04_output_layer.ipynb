{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac63765",
   "metadata": {},
   "source": [
    "# 출력층 설계 (Output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5bea676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.68941421e-01 7.31058579e-01 3.72187099e-88]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# np.exp(1000) 이것만 해도 오버플로우나서 max로 빼야함\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "x = np.array([1000, 1001, 800])\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7f91c",
   "metadata": {},
   "source": [
    "- PyTorch 라이브러리 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b11f59d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652], dtype=torch.float64)\n",
      "tensor([1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# x = torch.tensor([1000, 1001, 1002], dtype=torch.float32)\n",
    "x = np.array([1000, 1001, 1002], dtype=float)\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "# softmax 를 쓰려면 float(실수) 이어야 함\n",
    "softmax_output = F.softmax(x, dim=0)\n",
    "print(softmax_output)\n",
    "\n",
    "print(torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917451e8",
   "metadata": {},
   "source": [
    "### 손실 함수와 연계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3586ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1523725986480713\n",
      "1.121687650680542\n",
      "1.0915027856826782\n",
      "1.0618436336517334\n",
      "1.0327346324920654\n",
      "1.0041998624801636\n",
      "0.9762616753578186\n",
      "0.9489403963088989\n",
      "0.9222531914710999\n",
      "0.8962143659591675\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 간단한 다중 클래스 분류 모델 정의\n",
    "class SimpleMultiClassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMultiClassModel, self).__init__()\n",
    "        self.fc = nn.Linear(5, 3) # fully connected\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "model = SimpleMultiClassModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 데이터 생성\n",
    "inputs = torch.randn(4, 5)\n",
    "labels = torch.tensor([0, 2, 1, 0])\n",
    "\n",
    "for _ in range(10):\n",
    "    preds = model(inputs)           # 순전파\n",
    "    loss = criterion(preds, labels) # 손실계산\n",
    "    print(loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()           # 기울기 초기화     이전단계에서 계산된 기울기를 0으로 초기화\n",
    "    loss.backward()                 # 역전파           손실에 대한 역전파 수행 파라미터에 대한 기울기 계산\n",
    "    optimizer.step()                # 가중치 업데이트   계산된 기울기를 사용하여 옵티마이저가 모델의 파라밈터 업뎃\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
